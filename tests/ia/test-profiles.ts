// tests/ia/test-profiles.ts
import { LLMFactory } from '@/infrastructure/adapters/llm/llm-factory';
import { getLlmConfigForProfile, type ModelProfile } from '@/infrastructure/config/env';

async function testProfile(profile: ModelProfile) {
  console.log(`\nüß™ Probando perfil: '${profile}'`);
  
  try {
    // 1. Obtener la configuraci√≥n para el perfil
    const config = getLlmConfigForProfile(profile);
    console.log(`   - Modelo seleccionado: ${config.model}`);

    // 2. Crear una instancia del LLM con esa configuraci√≥n
    const llm = LLMFactory.create(config.provider, {
      apiKey: config.apiKey,
      model: config.model,
    });

    // 3. Realizar una pregunta simple para verificar que funciona
    const question = `Confirma que est√°s usando el modelo ${config.model} y dime para qu√© perfil est√°s configurado.`;
    const response = await llm.generateResponse(question, undefined, {
      temperature: 0.1,
      maxTokens: 100,
    });

    console.log(`   - ‚úÖ Respuesta del modelo: "${response.content.trim()}"`);
  } catch (error) {
    console.error(`   - ‚ùå Error probando el perfil '${profile}':`, error instanceof Error ? error.message : error);
  }
}

async function runAllProfileTests() {
  console.log('================================================');
  console.log('ü§ñ Ejecutando pruebas de perfiles de modelo...');
  console.log('================================================');
  await testProfile('fast');
  await testProfile('powerful');
  await testProfile('dev');
}

runAllProfileTests();