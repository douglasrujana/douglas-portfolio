// src/core/ports/ILLMProvider.ts

/**
 * Interface para proveedores de LLM (Large Language Models)
 * 
 * Esta interface define el contrato que cualquier proveedor de LLM debe cumplir.
 * Siguiendo el Dependency Inversion Principle (DIP), las capas superiores
 * dependen de esta abstracci贸n, no de implementaciones concretas.
 * 
 * Soporta: Gemini, OpenAI, Claude, y cualquier otro LLM futuro.
 */
export interface ILLMProvider {
  /**
   * Genera una respuesta basada en un prompt
   * @param prompt - El mensaje del usuario
   * @param context - Contexto adicional (opcional)
   * @param options - Opciones de generaci贸n (opcional)
   * @returns Respuesta del LLM con metadata
   */
  generateResponse(
    prompt: string,
    context?: string[],
    options?: LLMOptions
  ): Promise<LLMResponse>;

  /**
   * Genera una respuesta en streaming (para UX m谩s fluida)
   * @param prompt - El mensaje del usuario
   * @param context - Contexto adicional (opcional)
   * @param options - Opciones de generaci贸n (opcional)
   * @returns AsyncIterable que emite chunks de texto
   */
  generateStream(
    prompt: string,
    context?: string[],
    options?: LLMOptions
  ): AsyncIterable<string>;

  /**
   * Estima el n煤mero de tokens en un texto
   * til para rate limiting y c谩lculo de costos
   * @param text - Texto a analizar
   * @returns N煤mero aproximado de tokens
   */
  estimateTokens(text: string): number;

  /**
   * Obtiene informaci贸n sobre el modelo actual
   * @returns Metadata del modelo
   */
  getModelInfo(): ModelInfo;
}

/**
 * Opciones de configuraci贸n para la generaci贸n
 */
export interface LLMOptions {
  /**
   * Temperature: controla la aleatoriedad (0 = determinista, 1 = creativo)
   * @default 0.7
   */
  temperature?: number;

  /**
   * N煤mero m谩ximo de tokens a generar
   * @default 1024
   */
  maxTokens?: number;

  /**
   * Top-p sampling: considera solo los tokens con probabilidad acumulada p
   * @default 0.95
   */
  topP?: number;

  /**
   * Secuencias que detienen la generaci贸n
   */
    stopSequences?: string[];
    
  //  AADE ESTA LNEA
  safetySettings?: any[]; // Puedes usar un tipo m谩s espec铆fico si lo importas

  /**
   * Penalizaci贸n por frecuencia (reduce repeticiones)
   * @default 0
   */
  frequencyPenalty?: number;

  /**
   * Penalizaci贸n por presencia (fomenta nuevos temas)
   * @default 0
   */
  presencePenalty?: number;
}

/**
 * Respuesta del LLM con metadata completa
 */
export interface LLMResponse {
  /**
   * Contenido de texto generado
   */
  content: string;

  /**
   * Tokens utilizados (prompt + completion)
   */
  tokensUsed: number;

  /**
   * Nombre del modelo utilizado
   */
  model: string;

  /**
   * Raz贸n de finalizaci贸n
   */
  finishReason?: 'stop' | 'length' | 'content_filter' | 'error';

  /**
   * Metadata adicional espec铆fica del proveedor
   */
  metadata?: {
    promptTokens?: number;
    completionTokens?: number;
    totalTokens?: number;
    latencyMs?: number;
    provider?: string;
    [key: string]: unknown;
  };

  /**
   * Timestamp de la generaci贸n
   */
  timestamp: Date;
}

/**
 * Informaci贸n sobre el modelo
 */
export interface ModelInfo {
  /**
   * Nombre del modelo
   */
  name: string;

  /**
   * Proveedor del modelo
   */
  provider: 'gemini' | 'openai' | 'claude' | string;

  /**
   * L铆mite m谩ximo de tokens de contexto
   */
  contextWindow: number;

  /**
   * Costo por 1M tokens de input (USD)
   */
  costPerMillionInputTokens?: number;

  /**
   * Costo por 1M tokens de output (USD)
   */
  costPerMillionOutputTokens?: number;

  /**
   * Capacidades del modelo
   */
  capabilities: {
    streaming: boolean;
    functionCalling: boolean;
    vision: boolean;
    json: boolean;
  };
}

/**
 * Errores espec铆ficos de LLM
 */
export class LLMError extends Error {
  constructor(
    message: string,
    public code: LLMErrorCode,
    public provider: string,
    public originalError?: unknown
  ) {
    super(message);
    this.name = 'LLMError';
  }
}

export enum LLMErrorCode {
  AUTHENTICATION_FAILED = 'AUTHENTICATION_FAILED',
  RATE_LIMIT_EXCEEDED = 'RATE_LIMIT_EXCEEDED',
  INVALID_REQUEST = 'INVALID_REQUEST',
  MODEL_NOT_FOUND = 'MODEL_NOT_FOUND',
  CONTENT_FILTERED = 'CONTENT_FILTERED',
  NETWORK_ERROR = 'NETWORK_ERROR',
  TIMEOUT = 'TIMEOUT',
  UNKNOWN = 'UNKNOWN',
}

/**
 * Utility type: Partial LLM Response para streaming
 */
export type StreamChunk = {
  delta: string;
  done: boolean;
};
